
Background
==========

My initial idea to distinguish between Malware and Humans
was based on an assumed difference in the patters of DNS
and then Web visits.  This turns out to be a poor assumption
and/or difficult to analyse based on local DNS caching 
(the browser remembers its lookups).  Also, quite alot
of malware does HTTP anyway.

So, new plan.

The data gathered shows that there are glaring time based
patterns from malware.  We are collecting data to the milli-
second, and the patterns are stark.

You get various patterns in DNS

* a small number of requests then a wait period, then the 
same small number
* a flood of quests with little wait period and then another
flood

There are more, and much more to be discovered.

The new idea is to identify malware based on time regular
behaviour.

If one were to graph each event on the X axis (time) with
a constant height per event, one would see regular spacings
between the events.

This is easily reduced by taking the differences between each
event and amalgamating them into buckets of time difference.

Simple Malware will show a ratio of common buckets to everything
else.  E.g a query twice then wait pattern will show the dominant
amount of 'nasty queries' occurring in two buckets of time difference.

This is a very simple analysis.

Design
======

New executable, to be invoked by cron or on demand, takes a fairly
large number of command-line switches and config options.  (Which
goes where will change as 'normal usage' emerges).

* time period to analyse
* data type (DNS or Web); i.e the table from which to extract data
* analyse by individual domains or amalgamate (all-in-one)
* bucket size (i.e the number of seconds in width for each bucket)
  [larger is better]
* min data points for considering any patterns 
  [larger is better]
* the tolerance (in milliseconds) for considering the next
  event to be 'close' to the previous.
  [smaller is better]

Basic Algorithm
===============

* Call the DB and get the data from the period, ordered by the 
  client identifier (MAC) and then the query domain.

Not sure whether to ignore the query domain or partition the 
analysis based on it.  This is tricky: partitioning and identifying
patterns in each may indicate multiple malware.  Is this valuable?
Lumping all query domains together may blur the distriction of patterns
for systems with lots of malware.  Perhaps in this case, just amount
of nasty queries should be the indicator ... ???

* parse the obtained data, by host identifier, into buckets of the 
  difference of time between each successive (*) record, and 
  analyse per host.  Round all times down to the bucket boundary.
  Fidelity is proportional to bucket size.  i.e grouping to 0.5 second
  buckets makes more buckets as offsets vary.

(*) one can also look at 2 step, 3 step, ... offsets, but this is 
more valuable when one has more than one independent influence
on the events. We are looking straight for dumb malware.

Analysis
========

We are looking for a few spikes against a minimum height (i.e
time differnce varied offset) background.

Picture the case of 4 queries rapidly every 11 minutes, and a 
bucket size of 2 seconds.  One will get a massive spike in the 
0-2 second bucket and a significant spike in the 558 - 660 second
bucket (there may be some spill over around the last).  The first
spike will be 3 times as large as the collection around 660 seconds.

Say we are looking at 1 day, during which the computer has been
actively connected the network for 2 periods of 3 hours.

This gives two lots of 15 'call home' attempts per hour.  
Thus, 30 * 3 events in the 0-2 second buffer and 30 * 1 events 
around the 660 second bucket.

Say the user, 6 times during the day visits a web site that is 
pulling in banner ads from dangerous domains, at 5 nasty banners
per visit.

Also assume that the malware is always contacting the same domain,
which is different to the banner add source.

On the domain partitioned analysis we have a very clear pattern.

On a combined analysis, the banner adds just add to the small bucket,
making the picture even more severe.

We expect very precise timing from the start of the 4 phase 'call
home' malware work, but a variable time between the 'casual browsing'
to nasty site human behaviour.

Thus, the domain partitioned analysis shows malware easily when
the malware contacts the same domain regularly, else we need to 
see the pattern in groups of the same number of calls offset by
very similar times.

THUS
====

We have clusters in time of events, either malware or 
visiting nasty sites (human) behaviour.  The offsets between
them are a first order indicator.

We have clusters in first offset of time events for both.

When re-arranging the graphs from buckets offsets listing 
in time difference, to grouping left on the graph by *height*
of bucket, we get a lovely logarithmic decline, in ALL cases.

The key thing is that there is a separation from the 'low background'
by this either human or malware driven network interaction.

It is the exageration of the peaks that matters.

First draft Algorithm
=====================

Run over the data, making differences and storing, but NOTE the
length of the repeated 'close' interval repeated queries.

-- Other idea --

Count these in 'buckets' of close events and also noting the 
times of the events.  e.g consequitive close events: 

  4,0,1,0,4,0,0,3,0,0,2,0,4,0,0,3 ... as buckets: 4 (4), 3 (2), 2, (1), 1 (1)

(ignore zero).  This gives a 'hint' to the cycle of the malware.

-- End other --

With the difference bucket (main) data, order decreasing in size,
traverse, looking for logarithmic form: note that a two cycle
gives two high bars followed by decay, all others give quick decay.

One must note the cumulative number of events 'included' whilst traversing.

It seems that all we need is a change from the total accumulated
to the next bucket being:

  more than half the events.

Yes. Ignore domains and just difference offset buck and find the 
logarithmic decline.

